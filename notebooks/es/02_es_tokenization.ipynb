{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.es.es_setup_methods import *\n",
    "from helpers.es.es_helper_methods import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5358480",
   "metadata": {},
   "source": [
    "## Word orianted tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2a651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to analyze\n",
    "text_to_analyze = \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone. Email me at john.smith@global-international.com\"\n",
    "\n",
    "# Tokenizers to test\n",
    "tokenizers = [\n",
    "    {\"tokenizer\": \"standard\"},\n",
    "    {\"tokenizer\": \"letter\"},\n",
    "    {\"tokenizer\": \"lowercase\"},\n",
    "    {\"tokenizer\": \"whitespace\"},\n",
    "    {\"tokenizer\": \"uax_url_email\"},\n",
    "    {\"tokenizer\": \"classic\"},\n",
    "]\n",
    "\n",
    "# Run analysis\n",
    "for tokenizer in tokenizers:\n",
    "\n",
    "    payload = {\n",
    "        \"text\": text_to_analyze,\n",
    "        \"tokenizer\": tokenizer['tokenizer']\n",
    "    }\n",
    "\n",
    "    tokens = analyze_text(payload=payload)\n",
    "\n",
    "    print(f\"--- {tokenizer['tokenizer']} tokenizer ---\")\n",
    "    print(tokens)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb71ca88",
   "metadata": {},
   "source": [
    "## Partial Word Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068ae3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to analyze\n",
    "text_to_analyze = \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n",
    "\n",
    "# Tokenizers to test\n",
    "tokenizers = [\n",
    "    {\"tokenizer\": \n",
    "     {\"type\": \"ngram\", \n",
    "      \"min_gram\": 3, \n",
    "      \"max_gram\": 3, \n",
    "      \"token_chars\": [\"letter\", \"digit\"]\n",
    "    }},\n",
    "    {\"tokenizer\": \n",
    "     {\"type\": \"edge_ngram\", \n",
    "      \"min_gram\": 1, \n",
    "      \"max_gram\": 3, \n",
    "      \"token_chars\": [\"letter\", \"digit\"]\n",
    "    }},\n",
    "]\n",
    "\n",
    "\n",
    "# Run analysis\n",
    "for tokenizer in tokenizers:\n",
    "\n",
    "    payload = {\n",
    "        \"text\": text_to_analyze,\n",
    "        \"tokenizer\": tokenizer['tokenizer']\n",
    "    }\n",
    "\n",
    "    tokens = analyze_text(payload=payload)\n",
    "\n",
    "    print(f\"--- {tokenizer['tokenizer']} tokenizer ---\")\n",
    "    print(tokens)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2569db14",
   "metadata": {},
   "source": [
    "## Structured Text Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d5ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to analyze\n",
    "text_to_analyze = \"The quick brown fox jumps over the lazy dog!\"\n",
    "\n",
    "# Tokenizers to test\n",
    "tokenizers = [\n",
    "    {\"tokenizer\": \"keyword\"},\n",
    "    {\"tokenizer\": \"pattern\"},\n",
    "    {\"tokenizer\": \"simple_pattern\"},\n",
    "    {\"tokenizer\": \"char_group\"},\n",
    "    {\"tokenizer\": \"simple_pattern_split\"},\n",
    "    {\"tokenizer\": \"path_hierarchy\"},\n",
    "]\n",
    "\n",
    "\n",
    "# Run analysis\n",
    "for tokenizer in tokenizers:\n",
    "\n",
    "    payload = {\n",
    "        \"text\": text_to_analyze,\n",
    "        \"tokenizer\": tokenizer['tokenizer']\n",
    "    }\n",
    "\n",
    "    tokens = analyze_text(payload=payload)\n",
    "\n",
    "    print(f\"--- {tokenizer['tokenizer']} tokenizer ---\")\n",
    "    print(tokens)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
